{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Data from the Web \n",
    "Kiran Bacsa, Samuel Beuret, and Valentine Santarelli\n",
    "\n",
    "## Important Notes\n",
    "* Make sure you push on GitHub your Notebook with all the cells already evaluated (i.e., you don't want your colleagues to generate unnecessary Web traffic during the peer review)\n",
    "* Don't forget to add a textual description of your thought process, the assumptions you made, and the solution you plan to implement!\n",
    "* Please write all your comments in English, and use meaningful variable names in your code.\n",
    "* You are not allowed to download manually the entire ranking -- rather you have to understand how the server loads it in your browser. \n",
    "\n",
    "\n",
    "Hints:\n",
    "- Keep your Notebook clean and don't print the verbose output of the requests if this does not add useful information for the reader.\n",
    "- In case of tie, use the order defined in the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import json \n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# set the default color palette\n",
    "# try other options: 'Blues', sns.cubehelix_palette(8)\n",
    "sns.set_palette('Blues')\n",
    "\n",
    "# Seaborn can also use a context for different purpose\n",
    "# possible values are paper, notebook, talk, and poster\n",
    "sns.set_context(\"notebook\")\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  200 top-ranking universities in www.topuniversities.com ([ranking 2018](https://www.topuniversities.com/university-rankings/world-university-rankings/2018)). \n",
    "\n",
    "### Scrapping the web page \n",
    "Using Postman, part of the information needed was found  in a JSON file (name, rank, country, and region). The remaining information to scrap ranking was found using Beautiful Soup on each university page.By inspecting the web page, we found that the information to scrap was in the *div* of *class = faculty-main*. \n",
    "* number of faculty members international corresponds to the *div* of *class=inter faculty*\n",
    "* number of faculty members total corresponds to the *div* of *class=total faculty*\n",
    "* number of students international corresponds to the *div* of *class=total inter*\n",
    "* number of students total corresponds to the *div* of *class=total student*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get name, rank, country and region of the 200 first universities\n",
    "\n",
    "#url found using Postman\n",
    "url='https://www.topuniversities.com/sites/default/files/qs-rankings-data/357051.txt?_=1508005996450'\n",
    "universities= requests.get(url).json()['data'][0:200] #take the first 200 universities\n",
    "#first part of the dataframe for topuniversities.com\n",
    "df_top1 = pd.DataFrame(universities)\n",
    "df_top1 = df_top1.drop(['cc','core_id','guide', 'logo','score','stars','nid'], axis=1) #don't need this information\n",
    "\n",
    "#list of urls to universities pages\n",
    "urls =list(df_top1['url'])\n",
    "\n",
    "# function to get the rest of the needed information on each university page in a dictionnary\n",
    "def get_details(url):\n",
    "    university= requests.get('https://www.topuniversities.com' + url)\n",
    "    soup= BeautifulSoup(university.text,'html.parser')\n",
    "    data= soup.find_all('div', class_= 'faculty-main') \n",
    "\n",
    "    # number of faculty members international\n",
    "    if(soup.find_all('div', class_ = 'inter faculty') != []):\n",
    "        inter_fac_mem = float(soup.find_all('div', class_ = 'inter faculty')[0].find_all('div', class_ = 'number')[0].string.replace(',',''))\n",
    "    else:\n",
    "        inter_fac_mem = float('nan')\n",
    "        \n",
    "    # number of faculty members total\n",
    "    if(soup.find_all('div', class_ = 'total faculty')!= []):\n",
    "        total_fac_mem = float(soup.find_all('div', class_ = 'total faculty')[0].find_all('div', class_ = 'number')[0].string.replace(',',''))\n",
    "    else:\n",
    "        total_fac_mem = float('nan')\n",
    "        \n",
    "    # number of students international\n",
    "    if(soup.find_all('div', class_ = 'total inter')!= []):\n",
    "        inter_student = float(soup.find_all('div', class_ = 'total inter')[0].find_all('div', class_ = 'number')[0].string.replace(',',''))\n",
    "    else:\n",
    "        inter_student = float('nan')\n",
    "        \n",
    "    # number of students total\n",
    "    if(soup.find_all('div', class_ = 'total student')!= []):\n",
    "        total_student = float(soup.find_all('div', class_ = 'total student')[0].find_all('div', class_ = 'number')[0].string.replace(',',''))\n",
    "    else:\n",
    "        total_student = float('nan')\n",
    "\n",
    "    #output dictionnary (keep the url to merge later)\n",
    "    details = {'url' : url, '(QS) #faculty members international' : inter_fac_mem,\n",
    "          '(QS) #faculty members total' : total_fac_mem,\n",
    "          '(QS) #students international': inter_student,\n",
    "          '(QS) #students total' : total_student}\n",
    "    return details\n",
    "\n",
    "#list of dictionnaries \n",
    "details = []    \n",
    "for url in urls:\n",
    "    details.append(get_details(url))\n",
    "#convert to pd.DataFrame\n",
    "df_top2 = pd.DataFrame(details)\n",
    "\n",
    "#merge the two dataframes using the url\n",
    "df_top= pd.merge(df_top1, df_top2, 'outer')\n",
    "df_top.rename(columns={'rank_display': '(QS) rank', 'title': 'name'}, inplace=True)\n",
    "df_top = df_top.drop('url', axis=1) #drop the url once the merge is done\n",
    "\n",
    "df_top.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to avoid scrapping the page every time we need the dataset *df_top*, the later was exported in a file using the pandas fucntion *to_pickle()*, and imported with *from_pickle()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USE PICKLE TO SAVE DF\n",
    "filename=\"./data/QS_ranking\"\n",
    "df_top.to_pickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USE PICKLE TO LOAD DF\n",
    "filename=\"./data/QS_ranking\"\n",
    "df_top =pd.read_pickle(filename)\n",
    "df_top.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We verify that there is no tie in the ranking and if so we keep the order of display on the web page.\n",
    "We found the ties using the *duplicated()* pandas function. Each time a duplicate is found (i.e a tie) we increment all the ranks below by one.\n",
    "As expected the rank column is unique at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_top['(QS) rank']=df_top['(QS) rank'].str.extract('(\\d+)').astype('int64')\n",
    "bool_dup = df_top['(QS) rank'].duplicated()\n",
    "ind_dup = bool_dup[bool_dup == True].index\n",
    "\n",
    "#until no more duplicates\n",
    "for ind in ind_dup :\n",
    "    #add +1 the all the ranks below this duplicate\n",
    "    df_top['(QS) rank'][ind : len(df_top['(QS) rank'])] = df_top['(QS) rank'][ind : len(df_top['(QS) rank']) ] + 1;\n",
    "    \n",
    "print(df_top['(QS) rank'].is_unique)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which are the best universities in term of: (a) ratio between faculty members and students, (b) ratio of international students?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the ratios to the df_top dataframe\n",
    "df_top['(QS) faculty members/students'] = df_top['(QS) #faculty members total']/df_top['(QS) #students total']\n",
    "df_top['(QS) international students/ total students'] = df_top['(QS) #students international']/df_top['(QS) #students total']\n",
    "df_top.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the ratio faculty members/students for the 10 first universities\n",
    "df_ratio1 = df_top.sort_values('(QS) faculty members/students', ascending =False) #sort by ratio\n",
    "df_ratio1.set_index(['name'],inplace=True) #set name of universities as index for the plot\n",
    "df_ratio1= df_ratio1['(QS) faculty members/students'] #extract only the column  needed\n",
    "\n",
    "ax = df_ratio1[0:10].plot.bar()\n",
    "ax.set_xlabel(\"Universities\", fontsize=12)\n",
    "plt.title(\"Ratio between faculty members and students\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best universities in term of ratio of faculty members and students are :\n",
    "* Caltech\n",
    "* Yale University\n",
    "* University of Oxford"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the ratio of international students for the first 10 universities\n",
    "df_ratio2 = df_top.sort_values('(QS) international students/ total students', ascending =False)#sort by ratio\n",
    "df_ratio2.set_index(['name'],inplace=True) #set name of universities as index for the plot\n",
    "df_ratio2= df_ratio2['(QS) international students/ total students']#extract only the column  needed\n",
    "\n",
    "ax = df_ratio2[0:10].plot.bar()\n",
    "ax.set_xlabel(\"Universities\", fontsize=12)\n",
    "plt.title(\"Ratio of international students\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best universities in term of ratio of international students are :\n",
    "* LSE\n",
    "* EPFL\n",
    "* Imperial College London\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which are the best country in term of: (a) ratio between faculty members and students, (b) ratio of international students?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new dataframe by aggregating values by country.\n",
    "df_country = df_top.groupby('country').mean()\n",
    "df_country.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sort by ratio and extract the columns needed\n",
    "df_country_ratio1 = df_country.sort_values('(QS) faculty members/students', ascending =False)\n",
    "df_country_ratio1= df_country_ratio1['(QS) faculty members/students']\n",
    "\n",
    "#Plot the sorted countries\n",
    "ax = df_country_ratio1[0:10].plot.bar()\n",
    "ax.set_xlabel(\"Country\", fontsize=12)\n",
    "plt.title(\"Ratio between faculty members and students\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best countries in term of ratio of faculty members and students are :\n",
    "* Russia\n",
    "* Denmark\n",
    "* Saudi Arabia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort by ratio and extract the columns needed\n",
    "df_country_ratio2 = df_country.sort_values('(QS) international students/ total students', ascending =False)\n",
    "df_country_ratio2= df_country_ratio2['(QS) international students/ total students']\n",
    "\n",
    "#Plot the sorted countries\n",
    "ax = df_country_ratio2[0:10].plot.bar()\n",
    "ax.set_xlabel(\"Country\", fontsize=12)\n",
    "plt.title(\"Ratio of international students\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best countries in term of ratio of international students are :\n",
    "* UK\n",
    "* Australia\n",
    "* Switzerland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which are the best region in term of: (a) ratio between faculty members and students, (b) ratio of international students?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new dataframe by aggregating values by region.\n",
    "df_region = df_top.groupby('region').mean()\n",
    "df_region.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort by ratio and extract the columns needed\n",
    "df_region_ratio1 = df_region.sort_values('(QS) faculty members/students', ascending =False)\n",
    "df_region_ratio1= df_region_ratio1['(QS) faculty members/students']\n",
    "\n",
    "#Plot the sorted regions\n",
    "ax = df_region_ratio1.plot.bar()\n",
    "ax.set_xlabel(\"Region\", fontsize=12)\n",
    "plt.title(\"Ratio between faculty members and students\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best regions in term of ratio of faculty members and students are :\n",
    "* North America\n",
    "* Asia\n",
    "* Europe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_region_ratio2 = df_region.sort_values('(QS) international students/ total students', ascending =False)\n",
    "df_region_ratio2= df_region_ratio2['(QS) international students/ total students']\n",
    "\n",
    "ax = df_region_ratio2.plot.bar()\n",
    "ax.set_xlabel(\"Region\", fontsize=12)\n",
    "plt.title(\"Ratio of international students\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best regions in term of ratio of international students are :\n",
    "* Oceania\n",
    "* Europe\n",
    "* North America"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  200 top-ranking universities in www.timeshighereducation.com ([ranking 2018](http://timeshighereducation.com/world-university-rankings/2018/world-ranking)).\n",
    "\n",
    "### Scrapping the web page "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url='https://www.timeshighereducation.com/sites/default/files/the_data_rankings/world_university_rankings_2018_limit0_369a9045a203e176392b9fb8f8c1cb2a.json'\n",
    "\n",
    "json_dict= requests.get(url).json()\n",
    "raw_data = pd.DataFrame(json_dict['data'])\n",
    "raw_data.columns\n",
    "df_time=raw_data[['name','rank','location','stats_number_students','stats_pc_intl_students','stats_student_staff_ratio']]\n",
    "\n",
    "df_time['rank']=df_time['rank'].str.extract('(\\d+)').astype('int64')\n",
    "df_time = df_time.loc[df_time['rank']<201]\n",
    "df_time.rename(columns={'location':'country','stats_number_students':'students','stats_pc_intl_students':'international students','stats_student_staff_ratio':'staff'},inplace=True)\n",
    "\n",
    "df_time['students'] = df_time['students'].str.replace(',','').astype('int64')\n",
    "\n",
    "df_time['international students'] = df_time['international students'].str.extract('(\\d+)').astype('float64')\n",
    "df_time['international students'] = (df_time['international students']/100)*df_time['students'].astype('float64')\n",
    "df_time['international students'] = df_time['international students'].astype('int64')\n",
    "\n",
    "df_time['staff'] = df_time['staff'].astype('float64')\n",
    "df_time['staff'] = df_time['students'].astype('float64')/df_time['staff']\n",
    "df_time['staff'] = df_time['staff'].astype('int64')\n",
    "\n",
    "df_time.rename(columns={'rank': '(Times) rank', 'title': 'name','staff':'(Times) #faculty members total', 'students':'(Times) #students total','international students':'(Times) #students international' }, inplace=True)\n",
    "df_time.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#USE PICKLE TO SAVE DF\n",
    "filename=\"./data/Times_ranking\"\n",
    "df_time.to_pickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USE PICKLE TO LOAD DF\n",
    "filename=\"./data/Times_ranking\"\n",
    "df_time =pd.read_pickle(filename)\n",
    "df_time.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time['(Times) faculty members/students']=df_time['(Times) #faculty members total'].astype('float64')/df_time['(Times) #students total']\n",
    "barplot = df_time.sort_values('(Times) faculty members/students',ascending=False)['(Times) faculty members/students'].head(10).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time['(Times) international students/ total students']=df_time['(Times) #students international'].astype('float64')/df_time['(Times) #students total']\n",
    "df_time.sort_values('(Times) international students/ total students',ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time.groupby(['country'])['(Times) faculty members/students'].mean().sort_values(ascending=False).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time.groupby(['country'])['(Times) international students/ total students'].mean().sort_values(ascending=False).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = {'Asia':['China','Hong Kong','Japan','Singapore','South Korea','Taiwan']\n",
    "          ,'Europe':['Austria','Belgium','Denmark','Finland','France','Germany','Ireland','Italy','Luxembourg','Netherlands','Norway','Russian Federation','Spain','Sweden','Switzerland','United Kingdom']\n",
    "          ,'North America':['Canada','United States']\n",
    "          ,'Africa':['South Africa']\n",
    "          ,'Oceania':['Australia','New Zealand']}\n",
    "\n",
    "region = { f : i for i in region for f in region[i] }\n",
    "\n",
    "df_time['region'] = df_time['country'].map(region)\n",
    "\n",
    "df_time.groupby(['region'])['(Times) faculty members/students'].mean().sort_values(ascending=False).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time.groupby(['region'])['(Times) international students/ total students'].mean().sort_values(ascending=False).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time.groupby(['region'])['(Times) rank'].count().sort_values(ascending=True).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging of the two DataFrames using university names. \n",
    "Match universities' names as well as you can, and explain your strategy. Keep track of the original position in both rankings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main problems that was observed was the fact that the Times ranking uses the universties' names expressed in their native language unlike the Qs-Ranking which uses the English name. The first step was therefore to clean our data by translating certain key words and caracters to their English equivalents. A specific translation was made for LMU Munich because its name in the QS-ranking was too distinct from its Times counterpart to be matched. Furthermore, redundant stop words that do not carry any relevant information such as 'The' and 'Of' were removed in order to make the university names more distinct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_target = [\"É\",\"é\",\"&\",\"Ludwig-Maximilians-Universität\",\"-\",\"Universite\",\"Universität\",\"Universitaet\",\"Universitat\",\"Technische\",\"Autònoma\",\"München\"]\n",
    "replace_with = [\"E\",\"e\",\"and\",\"LMU\",\" \",\"University\",\"University\",\"University\",\"University\",\"Technical\",\"Autonomous\",\"Munich\"]\n",
    "remove = [\"The \",\"the \",\"Of \",\"of \"]\n",
    "\n",
    "for i in range(len(replace_target)):\n",
    "    df_top['name'] = df_top['name'].str.replace(replace_target[i], replace_with[i])\n",
    "    df_time['name'] = df_time['name'].str.replace(replace_target[i], replace_with[i])\n",
    "\n",
    "for word in remove:\n",
    "    df_top['name'] = df_top['name'].str.replace(word, \"\")\n",
    "    df_time['name'] = df_time['name'].str.replace(word, \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then define the functions that will be used to match the university names between each other. The first step in each function groups the universities by country. This operation considerably reduces the number of possible false positives during the matching phase. During this matching phase, for each country, we compare the similarity of the universities between themselves. If this similarity is above a predefined threshold, both universities are considered to be a match and their indexes are paired together. These universities are then removed from the symmetric difference datasets. The first function uses the SequenceMatcher similarity. This similarity is defined as 2.0\\*M/T, where T is the total number of elements in both sequences, and M is the number of matches. The second function defines the similarity as the sum of the ratios of the number of common words versus the length of the universities' names (this function was mainly defined in order to compare similar names with different word orders). Both these functions return the dataframes containing the universities that haven't been paired up as well as the list of the paired up indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# match universities according to name similarity\n",
    "def find_common_unmerged(df_top_out, df_time_out, threshold):\n",
    "    n_duplicates = 0\n",
    "    matched_top = []\n",
    "    matched_time = []\n",
    "    \n",
    "    top_grouped = df_top_out.groupby('country')\n",
    "    time_grouped = df_time_out.groupby('country')\n",
    "\n",
    "    countries = pd.Series(list(set(df_top_out['country']).intersection(set(df_time_out['country']))))\n",
    "    \n",
    "    for country in countries:        \n",
    "        unis_top = top_grouped.get_group(name=country)\n",
    "        unis_time = time_grouped.get_group(name=country)     \n",
    "        for name in unis_top['name']:\n",
    "            for compare in unis_time['name']:\n",
    "                score = SequenceMatcher(None, name, compare).ratio()\n",
    "                if score > threshold:\n",
    "                    n_duplicates = n_duplicates + 1\n",
    "                    print('Matched ', name, ' with ', compare, ' with score ',score)\n",
    "                    matched_top.extend(df_top_out.index[df_top_out['name'] == name])\n",
    "                    matched_time.extend(df_time_out.index[df_time_out['name'] == compare])\n",
    "                    \n",
    "    print('Number of duplicates found : ', n_duplicates, 'out of ', len(df_top_out['name']), ' unmerged universities.')\n",
    "    df_top_out = df_top_out.drop(matched_top, axis=0)\n",
    "    df_time_out = df_time_out.drop(matched_time, axis=0)\n",
    "    return df_top_out, df_time_out, matched_top, matched_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# match universities according to common substring\n",
    "def find_common_unmerged_by_substrings(df_top_out, df_time_out, threshold):\n",
    "    n_duplicates = 0\n",
    "    matched_top = []\n",
    "    matched_time = []\n",
    "    \n",
    "    top_grouped = df_top_out.groupby('country')\n",
    "    time_grouped = df_time_out.groupby('country')\n",
    "\n",
    "    countries = pd.Series(list(set(df_top_out['country']).intersection(set(df_time_out['country']))))\n",
    "    \n",
    "    for country in countries:        \n",
    "        unis_top = top_grouped.get_group(name=country)\n",
    "        unis_time = time_grouped.get_group(name=country)     \n",
    "        for name in unis_top['name']:\n",
    "            for compare in unis_time['name']:\n",
    "                top_name_words = name.split()\n",
    "                time_name_words = compare.split()\n",
    "                common = set(top_name_words).intersection( set(time_name_words) )\n",
    "                score = len(common)/len(top_name_words) + len(common)/len(time_name_words)\n",
    "                if score > threshold:\n",
    "                    n_duplicates = n_duplicates + 1\n",
    "                    print('Matched ', name, ' with ', compare, ' with score ',score)\n",
    "                    matched_top.extend(df_top_out.index[df_top_out['name'] == name])\n",
    "                    matched_time.extend(df_time_out.index[df_time_out['name'] == compare])\n",
    "                    \n",
    "    print('Number of duplicates found : ', n_duplicates, 'out of ', len(df_top_out['name']), ' unmerged universities.')\n",
    "    df_top_out = df_top_out.drop(matched_top, axis=0)\n",
    "    df_time_out = df_time_out.drop(matched_time, axis=0)\n",
    "    return df_top_out, df_time_out, matched_top, matched_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run these functions with different thresholds on the subdataframes that contain the symmetric differences of both datasets with respect to names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get universities that weren't merged\n",
    "df_top_out = df_top[~df_top['name'].isin(df_time['name'])]\n",
    "df_time_out = df_time[~df_time['name'].isin(df_top['name'])]\n",
    "\n",
    "total_matched_top = []\n",
    "total_matched_time = []\n",
    "m_top = []\n",
    "m_time = []\n",
    "\n",
    "# threshold : 0.9\n",
    "df_top_out, df_time_out, m_top, m_time = find_common_unmerged(df_top_out, df_time_out, 0.9)\n",
    "total_matched_top.extend(m_top)\n",
    "total_matched_time.extend(m_time)\n",
    "del m_top[:]\n",
    "del m_time[:]\n",
    "\n",
    "# threshold : 0.85\n",
    "df_top_out, df_time_out, m_top, m_time = find_common_unmerged(df_top_out, df_time_out, 0.85)\n",
    "total_matched_top.extend(m_top)\n",
    "total_matched_time.extend(m_time)\n",
    "del m_top[:]\n",
    "del m_time[:]\n",
    "\n",
    "# by substrings\n",
    "df_top_out, df_time_out, m_top, m_time = find_common_unmerged_by_substrings(df_top_out, df_time_out, 1.4)\n",
    "total_matched_top.extend(m_top)\n",
    "total_matched_time.extend(m_time)\n",
    "del m_top[:]\n",
    "del m_time[:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have detected the university pairs, we can change the Times ranking names to their QS ranking equivalents and merge both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace time names with their top equivalent : \n",
    "for i in range(len(total_matched_time)):\n",
    "    df_time.loc[total_matched_time[i],'name'] = df_top.loc[total_matched_top[i],'name']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_merged = pd.merge(df_top, df_time, how='outer')\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory analysis\n",
    "* Find useful insights in the data by performing an exploratory analysis. Can you find a strong correlation between any pair of variables in the dataset you just created? Example: when a university is strong in its international dimension, can you observe a consistency both for students and faculty members?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "#TO CHANGE LATER\n",
    "# merge 'in' universities with same string name\n",
    "\n",
    "df_merged = pd.merge(df_top, df_time, how='outer')\n",
    "corr_pearson= df_merged.corr('pearson')\n",
    "\n",
    "sns.heatmap(corr_pearson, \n",
    "            xticklabels=corr_pearson.columns.values,\n",
    "            yticklabels=corr_pearson.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " correlation  between\n",
    " \n",
    "* faculty member international, faculty members total\n",
    "* faculty member international, student international\n",
    "* faculty member international, ratio fac/student\n",
    "* faculty member international, international students (time)\n",
    "* student international, ratio international students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corr_spearman= df_merged.corr(method='spearman')\n",
    "\n",
    "sns.heatmap(corr_spearman, \n",
    "            xticklabels=corr_spearman.columns.values,\n",
    "            yticklabels=corr_spearman.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corr_kendall= df_merged.corr('kendall')\n",
    "\n",
    "sns.heatmap(corr_kendall, \n",
    "            xticklabels=corr_kendall.columns.values,\n",
    "            yticklabels=corr_kendall.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_check= df_merged[[ '(QS) rank', '#faculty members total (QS)',\n",
    "        '#students total (QS)',\n",
    "       '#students international (QS)',\n",
    "       'rank','staff', 'students',\n",
    "       'international students']]\n",
    "df_check = df_check.rename(columns={'QS rank': '(QS) rank','#faculty members total (QS)': '(QS) #faculty members total','#students total (QS)':'(QS) #students total', '#students international (QS)': '(QS) #students international' })\n",
    "df_check = df_check.rename(columns={'rank': '(Times) rank ', 'staff': '(Times) #faculty members total','students':'(Times) #students total', 'international students':'(Times) #students international' });\n",
    "corr_diff = df_check.corr();\n",
    "\n",
    "test=corr_diff[['(QS) rank', '(QS) #faculty members total', '(QS) #students total',\n",
    "       '(QS) #students international']].loc[\n",
    "        '(Times) rank ':];\n",
    "sns.heatmap(test)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the best university"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Can you find the best university taking in consideration both rankings? Explain your approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the best university, we decided to use a Condorcet method. We group the univeristies two by two and we perform a vote (each university has a vote from one of the ranking if its score is higher than the other one). The overall score of the university is the number of vote it wins.\n",
    "\n",
    "It has a drawback since it many of the univerities are tied in ranking. So we tried another to counter this effect by also considering the Shanghai ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Parsing the dataset of the Shanghai ranking\n",
    "\n",
    "# get name, rank, country and region of the 200 first universities\n",
    "\n",
    "\n",
    "\n",
    "# function to get the rest of the needed information on each university page in a dictionnary\n",
    "'''\n",
    "universities= requests.get('http://www.shanghairanking.com/ARWU2017.html')\n",
    "soup= BeautifulSoup(universities.text,'html.parser')\n",
    "html_shanghai= soup.find_all('table', id=\"UniversityRanking\")\n",
    "rank_shanghai_even= html_shanghai[0].find_all('tr', class_=\"bgfd\")\n",
    "rank_shanghai_odd= html_shanghai[0].find_all('tr', class_=\"bgf5\")\n",
    "\n",
    "uni_list=[]\n",
    "for uni in rank_shanghai_even[0:50]:\n",
    "    uni_rank=int(uni.find('td').text);\n",
    "    uni_name=uni.find('a').text\n",
    "    uni_url='http://www.shanghairanking.com/' + uni.find('a')['href']; \n",
    "    uni_request= requests.get(uni_url)\n",
    "    uni_soup= BeautifulSoup(uni_request.text,'html.parser')\n",
    "    uni_country= uni_soup.find('div', class_=\"tab_content\").find_all('td')[5].text\n",
    "    uni_region= uni_soup.find('div', class_=\"tab_content\").find_all('td')[3].text\n",
    "    uni_list.append({'name' : uni_name, 'rank Shanghai' : uni_rank,\n",
    "          'country' : uni_country,\n",
    "          'region': uni_region})\n",
    "\n",
    "    \n",
    "for uni in rank_shanghai_odd[0:50]:\n",
    "    uni_rank=int(uni.find('td').text);\n",
    "    uni_name=uni.find('a').text\n",
    "    uni_url='http://www.shanghairanking.com/' + uni.find('a')['href']; \n",
    "    uni_request= requests.get(uni_url)\n",
    "    uni_soup= BeautifulSoup(uni_request.text,'html.parser')\n",
    "    uni_country= uni_soup.find('div', class_=\"tab_content\").find_all('td')[5].text\n",
    "    uni_region= uni_soup.find('div', class_=\"tab_content\").find_all('td')[3].text\n",
    "    uni_list.append({'name' : uni_name, 'rank Shanghai' : uni_rank,\n",
    "          'country' : uni_country,\n",
    "          'region': uni_region})\n",
    "\n",
    "df_shanghai=pd.DataFrame(uni_list)\n",
    "\n",
    "df_shanghai=df_shanghai.sort_values('rank Shanghai', ascending=True)\n",
    "df_shanghai.head()\n",
    "\n",
    "df_shanghai.index.is_unique\n",
    "df_shanghai.set_index('rank Shanghai')\n",
    "\n",
    "#USE PICKLE TO SAVE DF\n",
    "filename=\"./data/Shanghai_ranking\"\n",
    "df_shanghai.to_pickle(filename)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading the dataset\n",
    "\n",
    "#USE PICKLE TO LOAD DF\n",
    "filename=\"./data/Shanghai_ranking\"\n",
    "df_shanghai =pd.read_pickle(filename)\n",
    "df_shanghai.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_shanghai['name']=df_shanghai['name'].str.lower()\n",
    "#df_shanghai=df_shanghai[['name','rank Shanghai']]\n",
    "\n",
    "\n",
    "\n",
    "country_target=['USA','UK']\n",
    "country_with=['United States','United Kingdom']\n",
    "\n",
    "for i in range(len(country_target)):\n",
    "    df_shanghai['country'] = df_shanghai['country'].str.replace(country_target[i], country_with[i])\n",
    "\n",
    "for i in range(len(replace_target)):\n",
    "    df_shanghai['name'] = df_shanghai['name'].str.replace(replace_target[i], replace_with[i])\n",
    "\n",
    "for word in remove:\n",
    "    df_shanghai['name'] = df_shanghai['name'].str.replace(word, \"\")\n",
    "    \n",
    "df_shanghai_out = df_shanghai[~df_shanghai['name'].isin(df_merged['name'])]\n",
    "df_merged_out = df_merged[~df_merged['name'].isin(df_shanghai['name'])]\n",
    "\n",
    "total_matched_merged = []\n",
    "total_matched_shanghai = []\n",
    "m_merged = []\n",
    "m_shanghai = []\n",
    "\n",
    "# threshold : 0.9\n",
    "df_merged_out, df_shanghai_out, m_merged, m_shanghai = find_common_unmerged(df_merged_out, df_shanghai_out, 0.9)\n",
    "total_matched_merged.extend(m_merged)\n",
    "total_matched_shanghai.extend(m_shanghai)\n",
    "del m_merged[:]\n",
    "del m_shanghai[:]\n",
    "\n",
    "# threshold : 0.85\n",
    "df_merged_out, df_shanghai_out, m_merged, m_shanghai = find_common_unmerged(df_merged_out, df_shanghai_out, 0.85)\n",
    "total_matched_merged.extend(m_merged)\n",
    "total_matched_shanghai.extend(m_shanghai)\n",
    "del m_merged[:]\n",
    "del m_time[:]\n",
    "\n",
    "# by substrings\n",
    "df_merged_out, df_shanghai_out, m_merged, m_shanghai = find_common_unmerged_by_substrings(df_merged_out, df_shanghai_out, 1.49)\n",
    "total_matched_merged.extend(m_merged)\n",
    "total_matched_shanghai.extend(m_shanghai)\n",
    "del m_merged[:]\n",
    "del m_shanghai[:]\n",
    "\n",
    "for i in range(len(total_matched_shanghai)):\n",
    "    df_shanghai.loc[total_matched_shanghai[i],'name'] = df_merged.loc[total_matched_merged[i],'name']\n",
    "\n",
    "df_shanghai=df_shanghai[['name','rank Shanghai']]\n",
    "    \n",
    "df_rank = pd.merge(df_merged, df_shanghai, how='outer')\n",
    "df_rank=df_rank[['name','rank','QS rank','rank Shanghai']]\n",
    "df_rank.columns=['name','TI rank','QS rank','SH rank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_vote(rank_serie):\n",
    "    rank_array = np.array(rank_serie)\n",
    "    rank_array[np.isnan(rank_array)]=201\n",
    "    rank_mat=np.tile(rank_array,(len(rank_array),1))\n",
    "    rank_cmp=np.less_equal(rank_mat,np.transpose(rank_mat)).astype(int)\n",
    "    return rank_cmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TI_cmp=compute_vote(df_rank['TI rank'])\n",
    "SH_cmp=compute_vote(df_rank['SH rank'])\n",
    "QS_cmp=compute_vote(df_rank['QS rank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tot_mat=np.greater(QS_cmp+TI_cmp,1).astype(int)\n",
    "df_rank['cond_score']= tot_mat.sum(axis=0)\n",
    "df_rank.sort_values(['cond_score'],ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here the results of the Condorcet method considering only the Times and QS rankings, we can see that we got a unique best university ("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tot_mat=np.greater(QS_cmp+TI_cmp+SH_cmp,1.5).astype(int)\n",
    "df_rank['cond_score']= tot_mat.sum(axis=0)\n",
    "df_rank.sort_values(['cond_score'],ascending=False).head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
